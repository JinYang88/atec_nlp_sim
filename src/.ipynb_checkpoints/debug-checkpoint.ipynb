{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import datahelper\n",
    "import jieba\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "test_mode = 0  # 0 for train+test 1 for test\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dim = 300\n",
    "hidden_dim = 128\n",
    "out_dim = 1\n",
    "\n",
    "epochs = 20\n",
    "print_every = 500\n",
    "bidirectional = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data..\n"
     ]
    }
   ],
   "source": [
    "print('Reading data..')\n",
    "ID = data.Field(sequential=False, batch_first=True, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, lower=True, eos_token='<EOS>', init_token='<BOS>',\n",
    "                  pad_token='<PAD>', fix_length=None, batch_first=True, use_vocab=True, tokenize=jieba.lcut)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data.TabularDataset(\n",
    "        path='../data/train.tsv', format='tsv',\n",
    "        fields=[('Id', ID), ('Text1', TEXT), ('Text2', TEXT), ('Label', LABEL)], skip_header=True)\n",
    "valid = data.TabularDataset(\n",
    "        path='../data/valid.tsv', format='tsv',\n",
    "        fields=[('Id', ID), ('Text1', TEXT), ('Text2', TEXT), ('Label', LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary Finished.\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('Building vocabulary Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data done.\n"
     ]
    }
   ],
   "source": [
    "train_iter = data.BucketIterator(dataset=train, batch_size=batch_size, sort_key=lambda x: len(x.Text1) + len(x.Text2), shuffle=True, device=device, repeat=False)\n",
    "valid_iter = data.Iterator(dataset=valid, batch_size=batch_size, device=device, shuffle=False, repeat=False)\n",
    "\n",
    "\n",
    "train_dl = datahelper.BatchWrapper(train_iter, [\"Text1\", \"Text2\", \"Label\"])\n",
    "valid_dl = datahelper.BatchWrapper(valid_iter, [\"Text1\", \"Text2\", \"Label\"])\n",
    "print('Reading data done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x1a18aefc30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_on(model, data_dl, loss_func, device ,model_state_path=None):\n",
    "    if model_state_path:\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "        print('Start predicting...')\n",
    "\n",
    "    model.eval()\n",
    "    res_list = []\n",
    "    label_list = []\n",
    "    loss = 0\n",
    "\n",
    "    \n",
    "    for text1, text2, label in data_dl:\n",
    "        y_pred = model(text1, text2)\n",
    "        loss += loss_func(y_pred, label).data.cpu()\n",
    "        y_pred = y_pred.data.max(1)[1].cpu().numpy()\n",
    "        res_list.extend(y_pred)\n",
    "        label_list.extend(label.data.cpu().numpy())\n",
    "        \n",
    "    acc = accuracy_score(res_list, label_list)\n",
    "    Precision = precision_score(res_list, label_list)\n",
    "    Recall = recall_score(res_list, label_list)\n",
    "    F1 = f1_score(res_list, label_list)\n",
    "\n",
    "    with open(\"res_list.txt\", 'w') as fw:\n",
    "        for item in res_list:\n",
    "            fw.write('{}\\n'.format(item))\n",
    "    \n",
    "    return loss, (acc, Precision, Recall, F1)\n",
    "\n",
    "\n",
    "class LSTM_angel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size, bidirectional):\n",
    "        super(LSTM_angel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.dist = nn.PairwiseDistance(2)\n",
    "    \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.word_embedding.weight.data.copy_(wordvec_matrix)\n",
    "#         self.word_embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2 if bidirectional else hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2 if bidirectional else hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        self.linear1 = nn.Linear(1, 200)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        # self.batchnorm1 = nn.BatchNorm1d(200)\n",
    "        self.linear2 = nn.Linear(200, 200)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        # self.batchnorm2 = nn.BatchNorm1d(200)\n",
    "        self.linear3 = nn.Linear(200, 200)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        # self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.linear4 = nn.Linear(200, 200)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "        # self.batchnorm4 = nn.BatchNorm1d(200)\n",
    "        self.linear5 = nn.Linear(200, 2)\n",
    "        \n",
    "    def forward(self, text1, text2, hidden_init=None) :\n",
    "        text1_word_embedding = self.word_embedding(text1)\n",
    "        text2_word_embedding = self.word_embedding(text2)\n",
    "#         print(text1)\n",
    "#         print(text1_word_embedding[0:3])\n",
    "        text1_seq_embedding = self.lstm_embedding(self.lstm1, text1_word_embedding, hidden_init)\n",
    "        text2_seq_embedding = self.lstm_embedding(self.lstm2, text2_word_embedding, hidden_init)\n",
    "#         print(\"------\")\n",
    "#         print(text1_seq_embedding[0][0:10])\n",
    "#         print(text2_seq_embedding[0][0:10])\n",
    "#         print(\"------\")\n",
    "        dot_value = torch.bmm(text1_seq_embedding.view(text1.size()[0], 1, self.hidden_dim), text2_seq_embedding.view(text1.size()[0], self.hidden_dim, 1))\n",
    "        dot_value = dot_value.view(text1.size()[0], 1)\n",
    "        # dist_value = self.dist(text1_seq_embedding, text2_seq_embedding).view(text1.size()[0], 1)\n",
    "#         print(dot_value)\n",
    "#         print(dist_value)\n",
    "#         feature_vec = torch.cat((text1_seq_embedding,text2_seq_embedding), dim=1)\n",
    "        # feature_vec = torch.cat((dot_value,dist_value), dim=1)\n",
    "#         print(feature_vec)\n",
    "#         sys.exit()\n",
    "        linearout_1 = self.linear1(dot_value)\n",
    "        linearout_1 = F.relu(linearout_1)\n",
    "        linearout_1 = self.dropout1(linearout_1)\n",
    "        # linearout_1 = self.batchnorm1(linearout_1)\n",
    "\n",
    "        linearout_2 = self.linear2(linearout_1)\n",
    "        linearout_2 = F.relu(linearout_2)\n",
    "        linearout_2 = self.dropout2(linearout_2)\n",
    "        # linearout_2 = self.batchnorm2(linearout_2)\n",
    "\n",
    "        linearout_3 = self.linear3(linearout_2)\n",
    "        linearout_3 = F.relu(linearout_3)\n",
    "        linearout_3 = self.dropout3(linearout_3)\n",
    "        # linearout_3 = self.batchnorm3(linearout_3)\n",
    "\n",
    "        linearout_4 = self.linear4(linearout_3)\n",
    "        linearout_4 = F.relu(linearout_4)\n",
    "        linearout_4 = self.dropout4(linearout_4)\n",
    "        # linearout_4 = self.batchnorm4(linearout_4)\n",
    "\n",
    "\n",
    "        linearout_5 = self.linear5(linearout_4)\n",
    "\n",
    "        return F.log_softmax(linearout_5, dim=1)\n",
    "    \n",
    "    def lstm_embedding(self, lstm, word_embedding ,hidden_init):\n",
    "        lstm_out,(lstm_h, lstm_c) = lstm(word_embedding, None)\n",
    "        if self.bidirectional:\n",
    "            seq_embedding = torch.cat((lstm_h[0], lstm_h[1]), dim=1)\n",
    "        else:\n",
    "            seq_embedding = lstm_h.squeeze(0)\n",
    "        return seq_embedding\n",
    "\n",
    "    def init_hidden(self, batch_size, device) :\n",
    "        layer_num = 2 if self.bidirectional else 1\n",
    "        if device == -1:\n",
    "            return (Variable(torch.randn(layer_num, batch_size, self.hidden_dim//layer_num)),Variable(torch.randn(layer_num, batch_size, self.hidden_dim//layer_num)))  \n",
    "        else:\n",
    "            return (Variable(torch.randn(layer_num, batch_size, self.hidden_dim//layer_num).cuda()),Variable(torch.randn(layer_num, batch_size, self.hidden_dim//layer_num).cuda()))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialing model..\n",
      "Start training..\n",
      "Evaluating....\n",
      "Saving model..\n",
      "Finish 500/1062 batch, 1/20 epoch. Time consuming 73.88s. acc is 0.77796483352, Loss is 88.5606613159\n",
      "Evaluating....\n",
      "Finish 1000/1062 batch, 1/20 epoch. Time consuming 158.46s. acc is 0.77796483352, Loss is 88.7370986938\n",
      "Evaluating....\n",
      "Finish 500/1062 batch, 2/20 epoch. Time consuming 267.59s. acc is 0.77796483352, Loss is 88.7696456909\n",
      "Evaluating....\n",
      "Finish 1000/1062 batch, 2/20 epoch. Time consuming 366.9s. acc is 0.77796483352, Loss is 88.6296157837\n",
      "Evaluating....\n",
      "Finish 500/1062 batch, 3/20 epoch. Time consuming 475.56s. acc is 0.77796483352, Loss is 88.6904373169\n",
      "Evaluating....\n",
      "Finish 1000/1062 batch, 3/20 epoch. Time consuming 571.68s. acc is 0.77796483352, Loss is 89.3532791138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fba78d568a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujinyang/anaconda2/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Initialing model..')\n",
    "MODEL = LSTM_angel(len(TEXT.vocab), embedding_dim, hidden_dim, batch_size, bidirectional=bidirectional)\n",
    "MODEL.to(device)\n",
    "    \n",
    "# print(MODEL.state_dict())\n",
    "\n",
    "# sys.exit()\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "\n",
    "# Train\n",
    "if not test_mode:\n",
    "    loss_func = nn.NLLLoss()\n",
    "    parameters = list(filter(lambda p: p.requires_grad, MODEL.parameters()))\n",
    "    optimizer = optim.Adam(parameters, lr=1e-3)\n",
    "    print('Start training..')\n",
    "\n",
    "    train_iter.create_batches()\n",
    "    batch_num = len(list(train_iter.batches))\n",
    "\n",
    "    batch_start = time.time()\n",
    "    for i in range(epochs) :\n",
    "        train_iter.init_epoch()\n",
    "        batch_count = 0\n",
    "        for text1, text2, label in train_dl:\n",
    "            MODEL.train()\n",
    "            y_pred = MODEL(text1, text2)\n",
    "            loss = loss_func(y_pred, label)\n",
    "            MODEL.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_count += 1\n",
    "            if batch_count % print_every == 0:\n",
    "                print(\"Evaluating....\")\n",
    "                loss, (acc, Precision, Recall, F1) = predict_on(MODEL, valid_dl, loss_func, device)\n",
    "                batch_end = time.time()\n",
    "                if acc > max_metric:\n",
    "                    best_state = MODEL.state_dict()\n",
    "                    max_metric = acc\n",
    "                    print(\"Saving model..\")\n",
    "                    torch.save(best_state, '../model_save/LSTM_angel.pth')           \n",
    "                print('Finish {}/{} batch, {}/{} epoch. Time consuming {}s. acc is {}, Loss is {}'.format(batch_count, batch_num, i+1, epochs, round(batch_end - batch_start, 2), acc, float(loss)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, (acc, Precision, Recall, F1) = predict_on(MODEL, valid_dl, nn.NLLLoss(), device)\n",
    "\n",
    "print(\"=================\")\n",
    "print(\"Evaluation results on test dataset:\")\n",
    "print(\"Loss: {}.\".format(float(loss)))\n",
    "print(\"Accuracy: {}.\".format(acc))\n",
    "print(\"Precision: {}.\".format(Precision))\n",
    "print(\"Recall: {}.\".format(Recall))\n",
    "print(\"F1: {}.\".format(F1))\n",
    "print(\"=================\")            "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
