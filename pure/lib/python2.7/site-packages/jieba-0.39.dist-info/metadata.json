{"classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Natural Language :: Chinese (Simplified)", "Natural Language :: Chinese (Traditional)", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Topic :: Text Processing", "Topic :: Text Processing :: Indexing", "Topic :: Text Processing :: Linguistic"], "description_content_type": "UNKNOWN", "extensions": {"python.details": {"contacts": [{"email": "ccnusjy@gmail.com", "name": "Sun, Junyi", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://github.com/fxsjy/jieba"}}}, "generator": "bdist_wheel (0.29.0)", "keywords": ["NLP", "tokenizing", "Chinese", "word", "segementation"], "license": "MIT", "metadata_version": "2.0", "name": "jieba", "summary": "Chinese Words Segementation Utilities", "version": "0.39"}